# ChurnGuard - Customer Churn Prediction System

ChurnGuard is a machine learning-powered web application that predicts which customers are likely to leave (churn) and provides actionable insights to help retain them. It's built for telecom companies but can be adapted for any subscription-based business.

## Why This Exists

Customer churn is expensive. It costs 5-25x more to acquire a new customer than to retain an existing one. ChurnGuard helps you:

- **Identify at-risk customers** before they leave
- **Understand why** they might churn using explainable AI
- **Get actionable recommendations** powered by Gemini AI
- **Process customers at scale** with batch analysis

Instead of waiting for customers to cancel, you can proactively reach out with the right offer at the right time.

## What It Does

### 1. Single Customer Analysis
Analyze one customer at a time to get:
- **Churn probability** (0-100%)
- **Risk classification** (High/Low)
- **SHAP explanations** showing which factors drive the prediction
- **AI-generated insights** suggesting specific retention actions

### 2. Batch Customer Analysis
Upload a CSV with hundreds or thousands of customers and get:
- **Bulk predictions** for all customers at once
- **High-risk customer identification**
- **Aggregate analysis** of common patterns
- **Strategic retention report** generated by AI
- **Downloadable results** with risk scores for each customer

### 3. Explainable AI
Unlike black-box models, ChurnGuard explains its predictions:
- **SHAP values** show exactly which features contribute to churn risk
- **Visual charts** make it easy to understand key drivers
- **Gemini AI** translates technical insights into human-readable recommendations

---

## How It Works (The Full Picture)

### Architecture Overview

```
┌─────────────┐        ┌──────────────┐        ┌─────────────┐
│   Frontend  │ ─────> │   Backend    │ ─────> │  ML Model   │
│  (Browser)  │ <───── │  (FastAPI)   │ <───── │ (LightGBM)  │
└─────────────┘        └──────────────┘        └─────────────┘
                              │
                              v
                       ┌──────────────┐
                       │  Gemini AI   │
                       │ (Insights)   │
                       └──────────────┘
```

**Data Flow:**
1. User enters customer data in the browser
2. Frontend sends HTTP request to FastAPI backend
3. Backend preprocesses the data (cleaning, encoding, scaling)
4. ML model predicts churn probability
5. SHAP calculates feature importance
6. Gemini AI generates human-readable insights
7. Results flow back to frontend and display visually

---

## Backend Architecture

The backend is built with **FastAPI** and organized into clear modules:

### Core Components

#### 1. **`routes.py`** - API Endpoints
The main entry point that handles all HTTP requests:

- **`POST /predict`**: Single customer prediction
  - Takes customer profile as JSON
  - Returns prediction, probability, SHAP values
  - Stores result for explainability endpoint

- **`POST /predict_batch_analysis`**: Batch processing
  - Accepts CSV file upload
  - Processes all customers
  - Generates AI strategy report
  - Returns predictions + report

- **`POST /explain_shap`**: AI insights
  - Uses the last prediction
  - Calls Gemini to explain why the customer might churn
  - Suggests retention action

**How it works:**
```python
# User data comes in
customer_data = {"gender": "Male", "tenure": 12, ...}

# Route calls prediction service
result = predict_single(model, customer_data)

# Result includes prediction, probability, SHAP values
return result
```

### Services Layer

These modules handle the business logic:

#### **`prediction_service.py`**
Manages all prediction operations:

- **`predict_single(model, customer_data)`**
  - Converts dict to DataFrame
  - Cleans the data
  - Makes prediction
  - Calculates SHAP values
  - Returns everything needed for display

- **`predict_batch(model, df)`**
  - Processes entire DataFrame
  - Identifies high-risk customers (probability > 0.6)
  - Calculates aggregate SHAP for the high-risk group
  - Returns results + SHAP insights

**Why separate this?** Keeps routes.py clean and makes prediction logic reusable.

#### **`shap_service.py`**
Handles explainability:

- **`calculate_shap_values(model, data)`**
  - Extracts preprocessor and classifier from the pipeline
  - Transforms data through preprocessing
  - Calculates SHAP values using TreeExplainer
  - Returns feature importance scores

- **`calculate_batch_shap_aggregate(model, high_risk_data)`**
  - Computes SHAP for all high-risk customers
  - Averages the values to find common drivers
  - Returns top 10 features affecting the entire group

**SHAP in simple terms:** If a customer has "Month-to-month contract" and it increases their churn risk by 0.35, SHAP tells you that. It's like showing your work in math class - it proves why the model made its decision.

#### **`ai_service.py`**
Integrates with Gemini AI:

- **`get_gemini_explanation(prompt)`**
  - Sends prompt to Gemini 2.5 Flash
  - Returns AI-generated text
  - Handles API communication

- **`generate_shap_explanation_prompt(...)`**
  - Creates a prompt for single customer analysis
  - Includes churn probability, top features, company policies
  - Asks Gemini to explain in plain language

- **`generate_batch_strategy_prompt(...)`**
  - Creates a prompt for batch analysis
  - Includes aggregate stats, SHAP insights
  - Asks for 3 actionable retention strategies

**Why use AI?** Raw SHAP values like "Contract: 0.35" don't mean much to a business user. Gemini translates that into "This customer is on month-to-month, which increases churn risk. Offer a 1-year contract with 15% discount."

#### **`batch_service.py`**
Handles batch-specific logic:

- **`analyze_high_risk_group(df, probs, shap_aggregate)`**
  - Filters customers with probability > 0.6
  - Calculates summary stats (average tenure, charges, common patterns)
  - Adds SHAP feature importance to the summary
  - Returns formatted string for AI prompt

### Utilities Layer

Reusable helper functions:

#### **`data_preprocessing.py`**
Data cleaning that matches training:

- **`clean_data(df)`**
  - Drops customerID (not needed for prediction)
  - Converts TotalCharges to numeric
  - Replaces "No internet service" with "No" (normalization)
  - Returns clean DataFrame ready for the model

**Critical:** Preprocessing must match exactly what was done during training. If you trained on normalized data but predict on raw data, the model will give garbage results.

#### **`model_loader.py`**
Simple model loading:

- **`load_model(path)`**
  - Loads the pickled model using joblib
  - Returns None with warning if file not found
  - Model is loaded once at startup, not on every request

#### **`extract.py`**
Loads company policies:

- **`extract_context(file_path)`**
  - Reads the policies.txt file
  - Returns as string
  - Used to provide business context to Gemini

**Why policies?** Gemini needs to know your company's constraints. You can't promise unlimited free service, so policies guide the AI to make realistic recommendations.

---

## Machine Learning Model

### Model Selection Process

We tested multiple algorithms in `cross_model_validator.ipynb`:
- Random Forest
- XGBoost
- LightGBM
- Neural Networks (Keras)

Each model was tested with different imbalance handling strategies:
- SMOTE (Synthetic Minority Over-sampling)
- KMeans SMOTE (Cluster-based oversampling)
- Class Weights (Loss function adjustment)

**Winner: LightGBM with Class Weights**

Why this combination won:
- Best F1 score on the test set
- Superior generalization on validation data
- Fast training and prediction
- Handles categorical features natively
- Works great with SHAP for explainability
- Memory efficient for large datasets
- No risk of overfitting to synthetic data

### Handling Class Imbalance

Churn datasets are imbalanced - most customers stay, few leave. Our dataset:
- **No Churn**: 5,163 customers (73.4%)
- **Churn**: 1,869 customers (26.6%)

**Approaches Tested:**

We experimented with three different techniques to handle this imbalance:

1. **SMOTE (Synthetic Minority Over-sampling)**
   - Creates synthetic examples of churners by interpolating between existing samples
   - Adds new training data points
   - Can lead to overfitting on synthetic patterns

2. **KMeans SMOTE**
   - Clusters the data first, then applies SMOTE within clusters
   - More sophisticated but computationally expensive
   - Risk of creating unrealistic synthetic samples

3. **Class Weights** ✓ **(Final Choice)**
   - Penalizes misclassifying the minority class more heavily
   - No synthetic data - uses only real customer examples
   - Built into LightGBM, computationally efficient
   - Better generalization on real-world data

**Why Class Weights Won:**

Class weights work by adjusting the loss function during training. When the model misclassifies a churner (minority class), it pays a higher penalty. This makes the model pay more attention to churners without generating fake data.

```python
# Pipeline uses class weights in the classifier
pipeline = Pipeline([
    ('preprocessor', ColumnTransformer(...)),
    ('classifier', LGBMClassifier(
        class_weight='balanced',  # Automatically adjusts weights
        random_state=42,
        verbose=-1
    ))
])
```

The `class_weight='balanced'` automatically calculates weights inversely proportional to class frequencies:
- **Churn weight**: Higher (makes model care more about catching churners)
- **No Churn weight**: Lower (less penalty for missing non-churners)

**Advantages of Class Weights:**
- ✅ Trains on real data only (no synthetic samples)
- ✅ Faster training (no extra data generation step)
- ✅ Better generalization to production data
- ✅ Simpler pipeline (one less preprocessing step)
- ✅ Native support in LightGBM (optimized implementation)

**Comparison Results:**

| Approach | F1 Score | Training Time | Risk of Overfitting | Complexity |
|----------|----------|---------------|---------------------|------------|
| SMOTE | ~0.82 | Slow | Medium | High |
| KMeans SMOTE | ~0.81 | Very Slow | High | Very High |
| **Class Weights** | **~0.84** | **Fast** | **Low** | **Low** |

Class weights consistently outperformed oversampling methods while being simpler and faster.

### Training Pipeline

The model is trained in `backend/model.ipynb`:

**Step 1: Data Loading & Cleaning**
```python
df = pd.read_csv("data/DataSet.csv")
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.dropna(subset=['TotalCharges'], inplace=True)
df.replace('No internet service', 'No', inplace=True)
```

**Step 2: Feature Engineering**
- Numerical features: StandardScaler (mean=0, std=1)
- Categorical features: OneHotEncoder (creates binary columns)

**Step 3: Pipeline Construction**
```python
pipeline = Pipeline([
    ('preprocessor', ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ])),
    ('classifier', LGBMClassifier(
        class_weight='balanced',  # Handles imbalance
        random_state=42,
        verbose=-1
    ))
])
```

**Step 4: Hyperparameter Tuning**
RandomizedSearchCV tests 20 combinations:
- `n_estimators`: [100, 200, 500]
- `learning_rate`: [0.01, 0.05, 0.1]
- `max_depth`: [-1, 10, 20]
- `num_leaves`: [20, 31, 50]
- `class_weight`: Always 'balanced'

Optimized for F1 score (balances precision and recall).

**Step 5: Model Saving**
```python
joblib.dump(best_model, 'data/best_churn_model.pkl')
```

The saved model includes the entire pipeline (preprocessing + classifier with class weights).

### Prediction Process

When predicting on new data:

**Step 1: Clean**
```python
df_clean = clean_data(df)  # Match training preprocessing
```

**Step 2: Extract Pipeline Components**
```python
preprocessor = model.named_steps['preprocessor']
classifier = model.named_steps['classifier']
```

**Step 3: Transform**
```python
X_transformed = preprocessor.transform(df_clean)
```

**Step 4: Predict**
```python
prediction = model.predict(df_clean)  # 0 or 1
probability = model.predict_proba(df_clean)[:, 1]  # 0.0 to 1.0
```

**Step 5: Explain**
```python
explainer = shap.TreeExplainer(classifier)
shap_values = explainer.shap_values(X_transformed)
```

### Features Used

**Customer Profile:**
- Gender, SeniorCitizen, Partner, Dependents, Tenure

**Services:**
- PhoneService, MultipleLines, InternetService
- OnlineSecurity, OnlineBackup, DeviceProtection
- TechSupport, StreamingTV, StreamingMovies

**Billing:**
- Contract (Month-to-month, One year, Two year)
- PaperlessBilling, PaymentMethod
- MonthlyCharges, TotalCharges

**19 features total** → After one-hot encoding → **~45 features** fed to the model

---

## Frontend Architecture

The frontend is a single-page application (SPA) with three views:

### File Structure

```
frontend/
├── index.html          # All HTML in one file, different sections
├── css/
│   └── styles.css     # All styles, organized by component
└── js/
    ├── config.js      # API_URL configuration
    ├── app.js         # Navigation and utilities
    ├── prediction.js  # Single prediction logic
    ├── batch.js       # Batch upload and processing
    └── chart.js       # SHAP chart rendering (Chart.js)
```

### How Frontend Works

#### **Navigation (`app.js`)**
Simple tab switching:
```javascript
function navigateTo(pageName) {
    // Hide all pages
    document.querySelectorAll('.page').forEach(page => {
        page.classList.remove('active');
    });
    
    // Show selected page
    document.getElementById(`${pageName}-page`).classList.add('active');
}
```

No routing library needed - just CSS display toggling.

#### **Single Prediction (`prediction.js`)**

**Flow:**
1. User fills form → clicks "Analyze Customer"
2. `handlePrediction()` gathers form data
3. Converts to JSON matching backend format
4. `fetch()` sends POST to `/predict`
5. `displayPredictionResults()` shows:
   - Risk badge (High/Low)
   - Circular probability gauge
   - SHAP bar chart
   - "Get AI Insights" button
6. Clicking insights → calls `/explain_shap`
7. Shows Gemini's recommendation

**Key function:**
```javascript
async function handlePrediction(event) {
    // Gather form data
    const customerData = {
        gender: "Male",
        tenure: 12,
        // ... all fields
    };
    
    // API call
    const response = await fetch(`${CONFIG.API_URL}/predict`, {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify(customerData)
    });
    
    const result = await response.json();
    displayPredictionResults(result);
}
```

#### **Batch Analysis (`batch.js`)**

**Flow:**
1. User clicks "Choose File" → selects CSV
2. `handleFileSelect()` triggers immediately
3. `processBatchFile()` creates FormData
4. `fetch()` uploads to `/predict_batch_analysis`
5. Backend processes (can take 10-30 seconds for large files)
6. `displayBatchResults()` shows:
   - AI-generated strategy report
   - Summary cards (total, high-risk, low-risk)
   - Download button for enriched CSV

**Key feature:** Loading state with spinner while backend processes.

#### **Charts (`chart.js`)**

Uses Chart.js to render SHAP values:
```javascript
function renderShapChart(featureNames, shapValues) {
    // Get top 5 by absolute value
    const top5 = combined.sort((a,b) => Math.abs(b) - Math.abs(a)).slice(0,5);
    
    // Color: red for positive (increases churn), green for negative
    const colors = data.map(val => val > 0 ? '#ef4444' : '#10b981');
    
    // Horizontal bar chart
    new Chart(ctx, {
        type: 'bar',
        indexAxis: 'y',  // Makes it horizontal
        // ... config
    });
}
```

### Design Principles

- **No frameworks**: Vanilla JavaScript keeps it simple
- **Responsive**: Works on mobile, tablet, desktop
- **Modern colors**: Blue/green/gray (no purple!)
- **Clear hierarchy**: Large headings, good spacing
- **Loading states**: Spinners and disabled buttons during API calls

---

## Data Flow Example

Let's trace a single prediction from start to finish:

### Step 1: User Input
User fills form:
```
Gender: Male
Tenure: 12 months
Contract: Month-to-month
Monthly Charges: $70
...
```

### Step 2: Frontend → Backend
```javascript
POST http://localhost:8000/predict
Body: {
    "gender": "Male",
    "tenure": 12,
    "Contract": "Month-to-month",
    "MonthlyCharges": 70.0,
    ...
}
```

### Step 3: Backend Receives
```python
@app.post("/predict")
async def predict_single_endpoint(customer: CustomerProfile):
    input_data = customer.model_dump()  # Pydantic → dict
    result = predict_single(model, input_data)
    return result
```

### Step 4: Prediction Service
```python
def predict_single(model, customer_data):
    df = pd.DataFrame([customer_data])
    df_clean = clean_data(df)  # Preprocessing
    
    pred = model.predict(df_clean)[0]        # 0 or 1
    prob = model.predict_proba(df_clean)[0][1]  # 0.78
    
    shap_vals = calculate_shap_values(model, df_clean)
    return {...}
```

### Step 5: SHAP Calculation
```python
def calculate_shap_values(model, preprocessed_data):
    preprocessor = model.named_steps['preprocessor']
    classifier = model.named_steps['classifier']
    
    X_transformed = preprocessor.transform(preprocessed_data)
    explainer = shap.TreeExplainer(classifier)
    shap_vals = explainer.shap_values(X_transformed)
    
    return shap_vals[1]  # Class 1 (churn)
```

### Step 6: Backend → Frontend
```json
{
    "prediction": 1,
    "churn_probability": 0.78,
    "shap_values": [0.02, -0.15, 0.35, ...],
    "feature_names": ["gender", "tenure", "Contract", ...]
}
```

### Step 7: Frontend Display
```javascript
displayPredictionResults(result);
// Shows:
// - "HIGH RISK" badge
// - 78% circular gauge
// - Bar chart: Contract (+0.35), Tenure (-0.15), ...
```

### Step 8: User Clicks "Get AI Insights"
```javascript
POST http://localhost:8000/explain_shap
```

### Step 9: AI Service
```python
prompt = f"""
Act as a Retention Manager.
Churn Risk: 78% (High)
Top Drivers:
- Contract: 0.35
- Tenure: -0.15
...
company context: {policies}

Briefly explain why they might leave and suggest 1 retention action.
"""

explanation = await get_gemini_explanation(prompt)
```

### Step 10: Gemini Response
```
This customer is high risk primarily due to their month-to-month 
contract, which provides no commitment. Their short tenure (12 months) 
indicates they haven't built loyalty yet.

Retention Action: Offer a 15% discount on a 1-year contract upgrade, 
emphasizing the savings and stability benefits.
```

### Step 11: Display Insights
Frontend shows Gemini's text in a green box with lightbulb icon.

---

## Running the Project

### Prerequisites

- Python 3.12+
- Google Gemini API key
- Modern web browser

### Step 1: Clone and Setup

```bash
# Clone repository
git clone <your-repo-url>
cd SuperVity

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install fastapi uvicorn pandas numpy scikit-learn \
    lightgbm shap joblib python-dotenv google-generativeai
```

### Step 2: Configure API Key

Create `.env` file in the project root:
```
GOOGLE_API_KEY=your_gemini_api_key_here
```

Get your key from: https://makersuite.google.com/app/apikey

### Step 3: Train Model (if needed)

If `backend/data/best_churn_model.pkl` doesn't exist:

```bash
cd backend
jupyter notebook model.ipynb
# Run all cells
```

This creates the trained model file.

### Step 4: Start Backend

```bash
cd backend
python3 routes.py
```

Backend runs on: http://localhost:8000

API docs: http://localhost:8000/docs

### Step 5: Start Frontend

Option A - Direct file:
```bash
open frontend/index.html  # macOS
start frontend/index.html  # Windows
xdg-open frontend/index.html  # Linux
```

Option B - Local server (recommended):
```bash
cd frontend
python3 -m http.server 3000
```
Visit: http://localhost:3000

### Step 6: Test It Out

**Single Prediction:**
1. Navigate to "Single Analysis"
2. Fill in customer details
3. Click "Analyze Customer"
4. View prediction and SHAP chart
5. Click "Get AI Insights"

**Batch Analysis:**
1. Navigate to "Batch Analysis"
2. Upload `backend/data/DataSet.csv` (or your own CSV)
3. Wait for processing
4. View strategy report
5. Download results

---

## Project Structure

```
SuperVity/
├── backend/
│   ├── data/
│   │   ├── best_churn_model.pkl   # Trained model
│   │   ├── DataSet.csv            # Training data
│   │   └── policies.txt           # Company policies
│   ├── services/
│   │   ├── prediction_service.py  # Prediction logic
│   │   ├── shap_service.py        # Explainability
│   │   ├── ai_service.py          # Gemini integration
│   │   └── batch_service.py       # Batch processing
│   ├── utils/
│   │   ├── data_preprocessing.py  # Data cleaning
│   │   ├── model_loader.py        # Model loading
│   │   └── extract.py             # Policy loading
│   ├── routes.py                  # FastAPI endpoints
│   └── model.ipynb                # Training notebook
├── frontend/
│   ├── css/
│   │   └── styles.css             # All styles
│   ├── js/
│   │   ├── config.js              # Configuration
│   │   ├── app.js                 # Main app logic
│   │   ├── prediction.js          # Single prediction
│   │   ├── batch.js               # Batch analysis
│   │   └── chart.js               # Charting
│   └── index.html                 # Main page
├── cross_model_validator.ipynb    # Model comparison
├── .env                           # API keys (gitignored)
└── README.md                      # This file
```

---

## Key Technologies

**Backend:**
- FastAPI - Modern Python API framework
- LightGBM - Gradient boosting model with built-in class weighting
- SHAP - Model explainability
- Gemini AI - Natural language insights
- Pandas/NumPy - Data manipulation
- Scikit-learn - ML preprocessing and pipelines

**Frontend:**
- Vanilla JavaScript - No frameworks
- Chart.js - Data visualization
- Modern CSS - Grid, Flexbox

---

## Common Issues & Solutions

### Backend won't start
- Check if port 8000 is in use: `lsof -ti:8000`
- Verify virtual environment is activated
- Check `.env` file exists with valid API key

### Model file not found
- Run `backend/model.ipynb` to train and save the model
- Check path: `backend/data/best_churn_model.pkl`

### CORS errors
- Make sure backend is running on `http://localhost:8000`
- Don't open HTML as `file://` - use a local server
- Backend CORS is configured to allow all origins

### Gemini API errors
- Verify API key in `.env`
- Check you have available quota
- Network firewall might block API calls

### SHAP calculation slow
- SHAP is computationally expensive
- For batch: only calculated on high-risk customers
- Consider caching if predicting same customer multiple times

---

## Future Improvements

- [ ] Add user authentication
- [ ] Store predictions in database
- [ ] Real-time monitoring dashboard
- [ ] A/B testing framework for retention strategies
- [ ] Email alerts for high-risk customers
- [ ] Integration with CRM systems
- [ ] Mobile app
- [ ] Multi-language support

---

## License

MIT License - feel free to use this for your own projects.

---

## Contributing

Pull requests welcome! Please:
1. Keep code clean and readable
2. Add comments for complex logic
3. Test your changes
4. Update this README if you add features

---

## Credits

Built with ❤️ using open-source tools and modern ML practices.

Special thanks to:
- SHAP for making ML interpretable
- Google Gemini for AI insights
- FastAPI for making backend development enjoyable
